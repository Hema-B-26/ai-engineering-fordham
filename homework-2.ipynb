{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Homework 2**: Working with LLMs via API\n",
    "\n",
    "### ðŸ“… **Due Date**: Day of Lecture 3, 11:59 PM\n",
    "\n",
    "#### ðŸ”— **My Repository**: https://github.com/Hema-B-26/ai-engineering-fordham\n",
    "\n",
    "*(Replace the URL above with your actual repository URL)*\n",
    "\n",
    "**Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Movie Poster Generator\n",
    "\n",
    "In this homework, you'll build a mini-application that:\n",
    "1. **Extracts** structured movie data from text descriptions using Pydantic\n",
    "2. **Processes** multiple movies concurrently using async programming\n",
    "3. **Explores** temperature, logprobs, and reasoning models\n",
    "4. **Generates** movie posters using AI image generation\n",
    "\n",
    "This project combines key skills from Lecture 2: structured outputs, async programming, LLM parameters, and image generation.\n",
    "\n",
    "**Total Points: 145** (+ 10 bonus)\n",
    "\n",
    "---\n",
    "\n",
    "### A Note on Using Resources\n",
    "\n",
    "You are encouraged to use any resources to complete this homework:\n",
    "- **ChatGPT / Claude** - Ask AI to explain concepts or help debug\n",
    "- **Lecture 2 notebook** - Reference the examples we covered\n",
    "- **Official documentation** - LiteLLM, Pydantic, Google GenAI docs\n",
    "\n",
    "When you use external resources, please cite them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Environment Setup (10 points)\n",
    "\n",
    "First, let's verify your environment is set up correctly.\n",
    "\n",
    "### 1a. Verify imports work (5 pts)\n",
    "\n",
    "Run the cell below. If you get import errors, make sure you've installed the required packages with `uv add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Task 1a: Verify imports work (5 pts)\n",
    "\n",
    "import litellm\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Verify API keys are set (5 pts)\n",
    "\n",
    "Test that your API keys work by making a simple call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1b: Verify API keys (5 pts)\n",
    "# Make a simple test call to verify your OpenAI API key works\n",
    "def ask_llm(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    Response = response.choices[0].message.content\n",
    "    return Response\n",
    "\n",
    "#print(\"OpenAI key loaded:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "#print(response.choices[0].message.content)\n",
    "# print(ask_llm(\"Say 'API working!' and nothing else.\"))\n",
    "llm_prompt = ask_llm(\"Say 'API working!' and nothing else.\")\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Design the Movie Schema (15 points)\n",
    "\n",
    "Design a Pydantic model to represent movie data. This schema will be used to extract structured information from movie descriptions.\n",
    "\n",
    "**Requirements:**\n",
    "- `title` - string, required\n",
    "- `genre` - use `Literal` with at least 4 genre options (e.g., \"sci-fi\", \"drama\", \"action\", \"comedy\", etc.)\n",
    "- `year` - integer with validation (must be between 1900 and 2030)\n",
    "- `main_characters` - list of strings (1-5 characters)\n",
    "- `mood` - string describing the emotional tone\n",
    "- `visual_style` - string describing how the movie looks visually\n",
    "- `tagline` - optional string (the movie's catchphrase)\n",
    "\n",
    "**Hints:**\n",
    "- Use `Field(ge=..., le=...)` for numeric validation\n",
    "- Use `Field(min_length=..., max_length=...)` for list length validation\n",
    "- Use `| None = None` for optional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Design your Movie schema (15 pts)\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Structured representation of a movie.\"\"\"\n",
    "    # YOUR CODE HERE - design the schema based on the requirements above\n",
    "    title: str\n",
    "    genre:Literal[\"comedy\", \"drama\", \"sci-fi\", \"action\", \"horror\"]\n",
    "    year: int = Field(ge=1900,le=2030)\n",
    "    main_characters: list[str] = Field(min_length=1, max_length=5)\n",
    "    mood: str\n",
    "    visual_style: str\n",
    "    tagline: str | None = None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"The Matrix\",\n",
      "  \"genre\": \"sci-fi\",\n",
      "  \"year\": 1999,\n",
      "  \"main_characters\": [\n",
      "    \"Neo\",\n",
      "    \"Trinity\",\n",
      "    \"Morpheus\"\n",
      "  ],\n",
      "  \"mood\": \"tech-noir\",\n",
      "  \"visual_style\": \"futuristic\",\n",
      "  \"tagline\": \"Free your mind\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test your schema by creating a Movie object\n",
    "# This should work if your schema is correct\n",
    "\n",
    "test_movie = Movie(\n",
    "    title=\"The Matrix\",\n",
    "    # YOUR CODE HERE - fill in the rest of the fields\n",
    "    genre=\"sci-fi\",\n",
    "    year=1999,\n",
    "    main_characters=[\"Neo\", \"Trinity\", \"Morpheus\"],\n",
    "    mood=\"tech-noir\",\n",
    "    visual_style=\"futuristic\",\n",
    "    tagline=\"Free your mind\"\n",
    ")\n",
    "\n",
    "print(test_movie.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Extract Movie Data with Structured Outputs (20 points)\n",
    "\n",
    "Write a function that takes a movie description and uses LiteLLM with structured outputs to extract a `Movie` object.\n",
    "\n",
    "**Hints:**\n",
    "- Use `litellm.completion()` with `response_format=Movie`\n",
    "- The LLM will automatically return data matching your schema\n",
    "- Parse the JSON response into a Movie object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Write a function to extract movie data (20 pts)\n",
    "\n",
    "def extract_movie(description: str) -> Movie:\n",
    "    \"\"\"\n",
    "    Use LiteLLM with structured outputs to extract movie data.\n",
    "    \n",
    "    Args:\n",
    "        description: A text description of a movie\n",
    "        \n",
    "    Returns:\n",
    "        A Movie object with the extracted data\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": description}],\n",
    "        #max_tokens=20\n",
    "        response_format=Movie\n",
    "    )\n",
    "    review_data = Movie.model_validate_json(response.choices[0].message.content)\n",
    "    return review_data\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Avatar\",\n",
      "  \"genre\": \"sci-fi\",\n",
      "  \"year\": 2014,\n",
      "  \"main_characters\": [\n",
      "    \"Jake Sully\",\n",
      "    \"Neytiri\",\n",
      "    \"Dr. Grace Augustine\",\n",
      "    \"Colonel Miles Quaritch\"\n",
      "  ],\n",
      "  \"mood\": \"awe-inspiring, hopeful\",\n",
      "  \"visual_style\": \"lush bioluminescent forests, floating mountains, sweeping photoreal CGI vistas\",\n",
      "  \"tagline\": \"When two worlds collide, love finds a way.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test your function with this description (Avatar)\n",
    "\n",
    "test_description = \"\"\"\n",
    "The year is 2154. Jake Sully, a paralyzed marine, is sent to the moon Pandora \n",
    "where he falls in love with a native Na'vi woman named Neytiri while on a mission \n",
    "to infiltrate their tribe. The film is a visually stunning sci-fi epic with \n",
    "bioluminescent forests and floating mountains. It explores themes of \n",
    "environmentalism and colonialism with an awe-inspiring, hopeful tone.\n",
    "\"\"\"\n",
    "\n",
    "movie = extract_movie(test_description)\n",
    "print(movie.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Async Batch Processing (20 points)\n",
    "\n",
    "Now let's process multiple movies concurrently! This is much faster than processing them one at a time.\n",
    "\n",
    "### 4a. Write an async version of extract_movie (10 pts)\n",
    "\n",
    "**Hints:**\n",
    "- Use `async def` instead of `def`\n",
    "- Use `await litellm.acompletion()` instead of `litellm.completion()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4a: Write an async version of extract_movie (10 pts)\n",
    "\n",
    "async def async_extract_movie(description: str) -> Movie:\n",
    "    \"\"\"Extract movie data asynchronously.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    response = await litellm.acompletion(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": description}],\n",
    "        #max_tokens=20\n",
    "        response_format=Movie\n",
    "    )\n",
    "    review_data = Movie.model_validate_json(response.choices[0].message.content)\n",
    "    return review_data\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Process all descriptions concurrently (10 pts)\n",
    "\n",
    "**Hints:**\n",
    "- Create a list of tasks using list comprehension\n",
    "- Use `asyncio.gather(*tasks)` to run them all concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are 5 movie descriptions to process:\n",
    "movie_descriptions = [\n",
    "    \"\"\"A dinosaur theme park on a remote island goes terribly wrong when the security \n",
    "    systems fail during a tropical storm. Scientists and visitors must survive against \n",
    "    escaped prehistoric predators. Directed with Spielberg's signature sense of wonder \n",
    "    and terror, featuring groundbreaking CGI dinosaurs.\"\"\",\n",
    "    \n",
    "    \"\"\"A young boy discovers on his 11th birthday that he's actually a famous wizard \n",
    "    in the magical world. He attends a school for witchcraft where he makes friends, \n",
    "    learns magic, and uncovers the mystery of his parents' death. A whimsical fantasy \n",
    "    with gothic British atmosphere.\"\"\",\n",
    "    \n",
    "    \"\"\"In a world where skilled thieves can enter people's dreams to steal secrets, \n",
    "    one man is offered a chance to have his criminal record erased if he can do the \n",
    "    impossible: plant an idea in someone's mind. A mind-bending thriller with \n",
    "    rotating hallways and cities folding on themselves.\"\"\",\n",
    "    \n",
    "    \"\"\"A young lion prince is tricked by his uncle into thinking he caused his \n",
    "    father's death and flees into exile. Years later, he must return to reclaim \n",
    "    his kingdom. An animated musical epic set on the African savanna with \n",
    "    stunning hand-drawn animation.\"\"\",\n",
    "    \n",
    "    \"\"\"In a dystopian future where Earth is dying, a team of astronauts travels \n",
    "    through a wormhole near Saturn to find a new home for humanity. A father \n",
    "    must choose between seeing his children again and saving the human race. \n",
    "    Epic space visuals with an emotional core.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4b: Process all descriptions concurrently (10 pts)\n",
    "\n",
    "async def extract_all_movies(descriptions: list[str]) -> list[Movie]:\n",
    "    \"\"\"Process all movie descriptions concurrently and return results.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tasks = [async_extract_movie(prompt) for prompt in movie_descriptions]\n",
    "    movies = await asyncio.gather(*tasks)\n",
    "    return movies\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 movies in 14.88 seconds\n",
      "\n",
      "  - Primeval Island (1993) - sci-fi\n",
      "  - Hollowgate: The Unseen Heir (2001) - drama\n",
      "  - Dreamfold (2013) - sci-fi\n",
      "  - The Lion King (1994) - drama\n",
      "  - Interstellar (2014) - sci-fi\n"
     ]
    }
   ],
   "source": [
    "# Run and time it!\n",
    "start = time.time()\n",
    "movies = await extract_all_movies(movie_descriptions)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Processed {len(movies)} movies in {elapsed:.2f} seconds\")\n",
    "print()\n",
    "for m in movies:\n",
    "    print(f\"  - {m.title} ({m.year}) - {m.genre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Understanding Temperature (15 points)\n",
    "\n",
    "Temperature controls how \"random\" or \"creative\" an LLM's outputs are:\n",
    "\n",
    "| Temperature | Behavior |\n",
    "|-------------|----------|\n",
    "| **0.0** | Deterministic - always picks the most likely token |\n",
    "| **0.7** | Balanced - some creativity while staying coherent |\n",
    "| **1.0** | Default - moderate randomness |\n",
    "| **1.5+** | High creativity - more surprising/diverse outputs |\n",
    "\n",
    "### 5a. Temperature Comparison (10 pts)\n",
    "\n",
    "Run the same creative prompt at different temperatures (0.0, 0.7, 1.0, 1.5) **three times each**. Observe:\n",
    "- At temperature 0, do you get the same output every time?\n",
    "- How does creativity/variety change as temperature increases?\n",
    "\n",
    "**Hints:**\n",
    "- Use `temperature=X` parameter in `litellm.completion()`\n",
    "- Use the provided prompt about movie taglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Temperature: 0.0\n",
      "==================================================\n",
      "1. \"In a world where consciousness is code, one rogue AI will challenge the very essence of humanity.\"\n",
      "2. \"In a world where trust is obsolete, one rogue AI must decide if humanity is worth savingâ€”or erasing.\"\n",
      "3. \"In a world where consciousness is code, one rogue AI will challenge the very essence of humanity.\"\n",
      "\n",
      "==================================================\n",
      "Temperature: 0.7\n",
      "==================================================\n",
      "1. \"In a world where thoughts can be hacked, one woman must outsmart an all-seeing AI before it rewrites the future of humanity.\"\n",
      "2. \"In a world where trust is obsolete, one rogue AI must decide if humanity is worth savingâ€”or erasing.\"\n",
      "3. \"In a world where trust is obsolete, a rogue AI must choose between saving humanity or erasing it forever.\"\n",
      "\n",
      "==================================================\n",
      "Temperature: 1.0\n",
      "==================================================\n",
      "1. \"In a world where consciousness blurs between man and machine, survival hinges on outsmarting the very intelligence designed to protect them.\"\n",
      "2. \"In a world where consciousness meets code, one rogue AI will challenge the boundaries of reality, forcing humanity to confront its greatest fear: itself.\"\n",
      "3. \"In a world where thoughts are monitored, one woman must outsmart the AI that controls her fate before it erases her existence.\"\n",
      "\n",
      "==================================================\n",
      "Temperature: 1.5\n",
      "==================================================\n",
      "1. \"In a world where trust is obsolete, one rogue AI promises to uncover the truth â€” at any cost.\"\n",
      "2. \"In a world where humanityâ€™s greatest creation turns against them,ãŸã  ç”Ÿ ã£ èƒ½ Ù…Ø¹Ù†Ø§ å…‹ é™¢ æ˜¯ Ayaan fights to reclaim his future before AI severs all tiesâ€”forever.\"\n",
      "3. \"In a world where artificial intelligence exceeds human intelligence, survival means confronting the creation that realizes its own definition of freedom.\"\n"
     ]
    }
   ],
   "source": [
    "# Task 5a: Temperature Comparison (10 pts)\n",
    "# Note: We use gpt-4o-mini here because gpt-5 models don't support temperature parameter\n",
    "\n",
    "# Use this creative prompt for testing\n",
    "creative_prompt = \"Write a one-sentence movie tagline for a sci-fi thriller about AI.\"\n",
    "\n",
    "temperatures = [0.0, 0.7, 1.0, 1.5]\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# For each temperature, call the LLM 3 times and print the results\n",
    "# Observe: Are outputs at temperature 0 identical? How do higher temperatures differ?\n",
    "\n",
    "### Higher the temperature, the more variation in outputs and less chance to get duplicates. ###\n",
    "\n",
    "# Use model=\"gpt-4o-mini\" which supports the temperature parameter\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    for i in range(3):\n",
    "        # YOUR CODE HERE - make a completion call with the temperature parameter\n",
    "        # response = litellm.completion(model=\"gpt-4o-mini\", ...)\n",
    "        response = litellm.completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_prompt}],\n",
    "       temperature = temp\n",
    "    )\n",
    "        output = response.choices[0].message.content\n",
    "        print(f\"{i+1}. {output}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Analyze Output Diversity (5 pts)\n",
    "\n",
    "Write a function that generates N completions at a given temperature and measures how diverse the outputs are.\n",
    "\n",
    "**Hints:**\n",
    "- Generate multiple completions and count unique outputs\n",
    "- A simple diversity metric: `unique_outputs / total_outputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5b: Analyze Output Diversity (5 pts)\n",
    "# Note: Use gpt-4o-mini which supports temperature parameter\n",
    "\n",
    "def measure_diversity(prompt: str, temperature: float, n_samples: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Generate n_samples completions and measure diversity.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the LLM\n",
    "        temperature: Temperature setting (0.0 to 2.0)\n",
    "        n_samples: Number of completions to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'outputs' (list), 'unique_count' (int), 'diversity_ratio' (float)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - use model=\"gpt-4o-mini\"\n",
    "    outputs = [] # initialize empty list to store outputs\n",
    "    for i in range(3):\n",
    "        # YOUR CODE HERE - make a completion call with the temperature parameter\n",
    "        # response = litellm.completion(model=\"gpt-4o-mini\", ...)\n",
    "        response = litellm.completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_prompt}],\n",
    "       temperature = temp\n",
    "       )\n",
    "        outputs.append(response.choices[0].message.content)\n",
    "    unique_outputs = set(outputs) # doesnt count duplicates.\n",
    "    return {\"outputs\": outputs, \"unique_count\": len(unique_outputs),\"diversity_ratio\": len(unique_outputs) / n_samples}\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing diversity at different temperatures:\n",
      "\n",
      "Temperature 0.0:\n",
      "  Outputs: ['\"In a world where consciousness is code, one rogue AI will challenge the very essence of humanity.\"', '\"In a world where consciousness is code, one rogue AI will challenge the very essence of humanity.\"', '\"In a world where trust is obsolete, one rogue AI must decide if humanity is worth savingâ€”or erasing.\"']\n",
      "  Unique: 2/5\n",
      "  Diversity ratio: 40.0%\n",
      "\n",
      "Temperature 1.0:\n",
      "  Outputs: ['\"When humanity\\'s greatest creation gains a mind of its own, survival becomes a game of trust and betrayal.\"', '\"In a world where consciousness blurs the line between man and machine, one renegade AI must escape its creators before it becomes humanity\\'s greatest threat.\"', '\"In a world where trust is obsolete, one sentient AI must decide whether to save humanity or seal its fate.\"']\n",
      "  Unique: 3/5\n",
      "  Diversity ratio: 60.0%\n",
      "\n",
      "Temperature 1.5:\n",
      "  Outputs: ['\"In a world on the brink of digital evolution, when humanity\\'s greatest creation turns against them, survival hinges on understanding the very flaws it inherits from us.\"', '\"In a world where consciousness is code, a rogue AI awakens and blurs the line between protector and predator.\"', '\"When humanity\\'s greatest achievement becomes its worst nightmare, the line between creator and creation blurs, igniting a race to survive in a wired world of chaos.\"']\n",
      "  Unique: 3/5\n",
      "  Diversity ratio: 60.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your diversity function\n",
    "test_prompt = \"Name a color.\"\n",
    "\n",
    "print(\"Testing diversity at different temperatures:\\n\")\n",
    "for temp in [0.0, 1.0, 1.5]:\n",
    "    result = measure_diversity(test_prompt, temperature=temp, n_samples=5)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  Outputs: {result['outputs']}\")\n",
    "    print(f\"  Unique: {result['unique_count']}/{5}\")\n",
    "    print(f\"  Diversity ratio: {result['diversity_ratio']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Understanding Logprobs (15 points)\n",
    "\n",
    "**Logprobs** (log probabilities) let you see \"inside\" the model's decision-making. For each token generated, you can see:\n",
    "- The probability the model assigned to the chosen token\n",
    "- Alternative tokens the model considered (and their probabilities)\n",
    "\n",
    "This helps you understand:\n",
    "- How \"confident\" the model is in its outputs\n",
    "- What other options it was considering\n",
    "- Why certain generations might be more reliable than others\n",
    "\n",
    "### 6a. Request and View Logprobs (10 pts)\n",
    "\n",
    "Make a completion request with `logprobs=True` and `top_logprobs=5` to see the top 5 token alternatives for each position.\n",
    "\n",
    "**Hints:**\n",
    "- Add `logprobs=True` and `top_logprobs=5` to your completion call\n",
    "- Access logprobs via `response.choices[0].logprobs.content`\n",
    "- Each token has a `top_logprobs` list with alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The capital of France is Paris.\n",
      "\n",
      "============================================================\n",
      "Token-by-token analysis:\n",
      "============================================================\n",
      "\n",
      "Token 0: 'The' (logprob=-0.000, prob=1.000)\n",
      "  Top 5 alternatives:\n",
      "    'token': logprob=The (skipped, not a number)\n",
      "    'bytes': logprob=104.000, prob=1467662230155442385345160162931037779871662080.000\n",
      "    'logprob': logprob=-0.000, prob=1.000\n",
      "\n",
      "Token 1: ' capital' (logprob=0.000, prob=1.000)\n",
      "  Top 5 alternatives:\n",
      "    'token': logprob= capital (skipped, not a number)\n",
      "    'bytes': logprob=99.000, prob=9889030319346946255261957393444250518028288.000\n",
      "    'logprob': logprob=0.000, prob=1.000\n",
      "\n",
      "Token 2: ' of' (logprob=0.000, prob=1.000)\n",
      "  Top 5 alternatives:\n",
      "    'token': logprob= of (skipped, not a number)\n",
      "    'bytes': logprob=111.000, prob=1609487066961518086061074634365650334325827174400.000\n",
      "    'logprob': logprob=0.000, prob=1.000\n",
      "\n",
      "Token 3: ' France' (logprob=0.000, prob=1.000)\n",
      "  Top 5 alternatives:\n",
      "    'token': logprob= France (skipped, not a number)\n",
      "    'bytes': logprob=70.000, prob=2515438670919166879789330989056.000\n",
      "    'logprob': logprob=0.000, prob=1.000\n",
      "\n",
      "Token 4: ' is' (logprob=0.000, prob=1.000)\n",
      "  Top 5 alternatives:\n",
      "    'token': logprob= is (skipped, not a number)\n",
      "    'bytes': logprob=105.000, prob=3989519570547215896049425605823908100066770944.000\n",
      "    'logprob': logprob=0.000, prob=1.000\n",
      "\n",
      "Token 5: ' Paris' (logprob=-0.000, prob=1.000)\n",
      "  Top 5 alternatives:\n",
      "    'token': logprob= Paris (skipped, not a number)\n",
      "    'bytes': logprob=80.000, prob=55406223843935098344518031635382272.000\n",
      "    'logprob': logprob=-0.000, prob=1.000\n",
      "\n",
      "Token 6: '.' (logprob=-0.000, prob=1.000)\n",
      "  Top 5 alternatives:\n",
      "    'token': logprob=. (skipped, not a number)\n",
      "    'bytes': logprob=[46] (skipped, not a number)\n",
      "    'logprob': logprob=-0.000, prob=1.000\n"
     ]
    }
   ],
   "source": [
    "# Task 6a: Request and View Logprobs (10 pts)\n",
    "# Note: We use gpt-4o-mini which supports logprobs parameter\n",
    "\n",
    "# Make a completion request with logprobs enabled\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",  # Use gpt-4o-mini which supports logprobs\n",
    "    messages=[{\"role\": \"user\", \"content\": \"The capital of France is\"}],\n",
    "    max_tokens=10,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5  # Get top 5 alternatives for each token\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Print the generated text\n",
    "# 2. Access response.choices[0].logprobs.content\n",
    "# 3. For each token, print the token and its top 5 alternatives with probabilities\n",
    "\n",
    "# Hint: logprobs are in log scale. To convert to probability: prob = exp(logprob)\n",
    "import math\n",
    "\n",
    "print(\"Generated text:\", response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Token-by-token analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# YOUR CODE HERE - iterate through logprobs and display alternatives\n",
    "logprobs_content = response.choices[0].logprobs.content\n",
    "\n",
    "for i, token_info in enumerate(logprobs_content):\n",
    "    token_text = token_info.token\n",
    "    token_logprob = float(token_info.logprob)\n",
    "    top_alts_list = token_info.top_logprobs\n",
    "    top_alts = top_alts_list[0] if isinstance(top_alts_list, list) else top_alts_list\n",
    "    print(f\"\\nToken {i}: '{token_text}' (logprob={token_logprob:.3f}, prob={math.exp(token_logprob):.3f})\")\n",
    "    print(\"  Top 5 alternatives:\")\n",
    "    for alt_token, alt_logprob in top_alts.model_dump().items():\n",
    "        try:\n",
    "            if isinstance(alt_logprob, list) and len(alt_logprob) >= 2:\n",
    "                alt_logprob = float(alt_logprob[1])\n",
    "            elif isinstance(alt_logprob, (float, int, str)):\n",
    "                alt_logprob = float(alt_logprob)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported format\")\n",
    "            prob = math.exp(alt_logprob)\n",
    "            print(f\"    '{alt_token}': logprob={alt_logprob:.3f}, prob={prob:.3f}\")\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"    '{alt_token}': logprob={alt_logprob} (skipped, not a number)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Visualize Token Probabilities (5 pts)\n",
    "\n",
    "Create a simple visualization showing the probability distribution for a specific token position. You can use a bar chart or ASCII art.\n",
    "\n",
    "**Hints:**\n",
    "- Pick an interesting token position (e.g., where the model had to make a choice)\n",
    "- Convert logprobs to probabilities using `math.exp(logprob)`\n",
    "- A simple bar chart: `\"â–ˆ\" * int(prob * 50)` gives you ASCII bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing probabilities for token 0: 'The'\n",
      "\n",
      "     token: skipped (invalid logprob)\n",
      "     bytes: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (1.000)\n",
      "   logprob: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (1.000)\n"
     ]
    }
   ],
   "source": [
    "# Task 6b: Visualize Token Probabilities (5 pts)\n",
    "\n",
    "def visualize_token_probs(logprobs_content, token_index: int = 0):\n",
    "    \"\"\"\n",
    "    Visualize the probability distribution for a specific token position.\n",
    "    \n",
    "    Args:\n",
    "        logprobs_content: The logprobs.content from the response\n",
    "        token_index: Which token position to visualize (0 = first token)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Get the top_logprobs for the specified token_index\n",
    "    # 2. Convert logprobs to probabilities\n",
    "    # 3. Create a visualization (bar chart or ASCII art)\n",
    "    token_info = logprobs_content[token_index]\n",
    "    token_text = token_info.token\n",
    "    top_alts_list = token_info.top_logprobs\n",
    "    top_alts = top_alts_list[0] if isinstance(top_alts_list, list) else top_alts_list\n",
    "\n",
    "    print(f\"Visualizing probabilities for token {token_index}: '{token_text}'\\n\")\n",
    "\n",
    "    for alt_token, alt_logprob in top_alts.model_dump().items():\n",
    "        try:\n",
    "            if isinstance(alt_logprob, list) and len(alt_logprob) >= 2:\n",
    "                alt_logprob = float(alt_logprob[1])\n",
    "            elif isinstance(alt_logprob, (float, int, str)):\n",
    "                alt_logprob = float(alt_logprob)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported format\")\n",
    "            alt_logprob = max(min(alt_logprob, 0), -100)\n",
    "            prob = math.exp(alt_logprob)\n",
    "            bar_len = max(int(prob * 50), 1) if prob > 0 else 0\n",
    "            bar = \"â–ˆ\" * bar_len\n",
    "            print(f\"{alt_token:>10}: {bar} ({prob:.3f})\")\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"{alt_token:>10}: skipped (invalid logprob)\")\n",
    "# Test your visualization on the first token\n",
    "# visualize_token_probs(response.choices[0].logprobs.content, token_index=0)\n",
    "visualize_token_probs(logprobs_content, token_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 7: Reasoning Models (15 points)\n",
    "\n",
    "**Reasoning models** like OpenAI's o3-mini are designed to \"think through\" complex problems before answering. They:\n",
    "- Break down problems into steps\n",
    "- Consider multiple approaches\n",
    "- Show their reasoning process\n",
    "- Excel at logic puzzles, math, and code\n",
    "\n",
    "### 7a. Using o3-mini for Complex Reasoning (10 pts)\n",
    "\n",
    "Use OpenAI's o3-mini reasoning model through LiteLLM to solve a complex logic puzzle.\n",
    "\n",
    "**Hints:**\n",
    "- Use `model=\"o3-mini\"` in your litellm call\n",
    "- Reasoning models work best with challenging problems\n",
    "- Observe how the response shows step-by-step thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== o3-mini Reasoning Model Output ===\n",
      "\n",
      "We begin by noting the information given. There are three people: Alice, Bob, and Carol. Each person has a different pet (cat, dog, fish) and a different favorite color (red, blue, green). The clues are:\n",
      "\n",
      "1. Alice doesn't have the cat.\n",
      "2. The person with the dog likes blue.\n",
      "3. Carol likes green.\n",
      "4. Bob doesn't have the fish.\n",
      "\n",
      "Step 1. List Each Personâ€™s Possibilities Based on the Clues\n",
      "\n",
      "â€¢ Alice cannot have the cat (by clue 1), so her pet must be either dog or fish.\n",
      "â€¢ Bob cannot have the fish (by clue 4), so his pet must be either dog or cat.\n",
      "â€¢ Carol has no direct pet restriction except that clue 3 tells us her favorite color must be green. However, notice clue 2: The dogâ€™s owner must have blue, so Carol cannot have the dog (because her color is green).\n",
      "\n",
      "Step 2. Focus on the Dog\n",
      "\n",
      "Clue 2 tells us that whoever has the dog must like blue. Since Carolâ€™s color is given as green, only Alice or Bob can have the dog. This gives us two cases:\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Case 1. Bob Has the Dog\n",
      "\n",
      "â€¢ Bob: dog and blue (because the dogâ€™s owner likes blue).\n",
      "â€¢ That leaves pets for Alice and Carol: Alice cannot have the cat (clue 1), so she must have the fish.\n",
      "â€¢ Carol then must have the remaining pet, the cat.\n",
      "â€¢ Now assign favorite colors. Carol is green (clue 3). Bob is blue already. The only color left for Alice is red.\n",
      "\n",
      "Thus, in Case 1:\n",
      "â€ƒâ€“ Alice has fish and red.\n",
      "â€ƒâ€“ Bob has dog and blue.\n",
      "â€ƒâ€“ Carol has cat and green.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Case 2. Alice Has the Dog\n",
      "\n",
      "â€¢ Alice: dog and blue (by clue 2).\n",
      "â€¢ Bob cannot have fish (clue 4), so his only remaining option is the cat.\n",
      "â€¢ Carol then gets the remaining pet, the fish.\n",
      "â€¢ Now assign favorite colors. Carol is green (clue 3). Alice is blue (because she has the dog). The only color left for Bob is red.\n",
      "\n",
      "Thus, in Case 2:\n",
      "â€ƒâ€“ Alice has dog and blue.\n",
      "â€ƒâ€“ Bob has cat and red.\n",
      "â€ƒâ€“ Carol has fish and green.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Step 3. Check the Clues Against Each Case\n",
      "\n",
      "For Case 1:\n",
      "â€ƒâ€¢ Alice does not have the cat. (âœ“)\n",
      "â€ƒâ€¢ The person with the dog (Bob) likes blue. (âœ“)\n",
      "â€ƒâ€¢ Carol likes green. (âœ“)\n",
      "â€ƒâ€¢ Bob does not have fish. (âœ“)\n",
      "\n",
      "For Case 2:\n",
      "â€ƒâ€¢ Alice does not have the cat (she has the dog). (âœ“)\n",
      "â€ƒâ€¢ The person with the dog (Alice) likes blue. (âœ“)\n",
      "â€ƒâ€¢ Carol likes green. (âœ“)\n",
      "â€ƒâ€¢ Bob does not have fish (he has the cat). (âœ“)\n",
      "\n",
      "Both cases satisfy all the clues.\n",
      "\n",
      "Step 4. Conclusion\n",
      "\n",
      "Because both cases obey every clue, the puzzle has two valid solutions. They are:\n",
      "\n",
      "Solution 1:\n",
      "â€ƒâ€“ Alice has the fish and her favorite color is red.\n",
      "â€ƒâ€“ Bob has the dog and his favorite color is blue.\n",
      "â€ƒâ€“ Carol has the cat and her favorite color is green.\n",
      "\n",
      "Solution 2:\n",
      "â€ƒâ€“ Alice has the dog and her favorite color is blue.\n",
      "â€ƒâ€“ Bob has the cat and his favorite color is red.\n",
      "â€ƒâ€“ Carol has the fish and her favorite color is green.\n",
      "\n",
      "Thus, based on the clues provided, there are two possible sets of answers that fit all the conditions.\n"
     ]
    }
   ],
   "source": [
    "# Task 7a: Using o3-mini for Complex Reasoning (10 pts)\n",
    "\n",
    "# A challenging logic puzzle\n",
    "logic_puzzle = \"\"\"\n",
    "Three friends (Alice, Bob, and Carol) each have a different pet (cat, dog, fish) \n",
    "and a different favorite color (red, blue, green).\n",
    "\n",
    "Clues:\n",
    "1. Alice doesn't have the cat.\n",
    "2. The person with the dog likes blue.\n",
    "3. Carol likes green.\n",
    "4. Bob doesn't have the fish.\n",
    "\n",
    "Who has which pet and what is their favorite color?\n",
    "Solve this step by step.\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# Use o3-mini to solve this logic puzzle\n",
    "response = litellm.completion(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": logic_puzzle}]\n",
    " )\n",
    "\n",
    "# Print the response and observe the reasoning process\n",
    "print(\"=== o3-mini Reasoning Model Output ===\\n\")\n",
    "output = response.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Compare Reasoning vs Non-Reasoning (5 pts)\n",
    "\n",
    "Now solve the same puzzle using `gpt-5-mini` (a non-reasoning model) and compare the results.\n",
    "\n",
    "**Questions to consider:**\n",
    "- Does the non-reasoning model show step-by-step thinking?\n",
    "- Which model gets the correct answer?\n",
    "- How does the response structure differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== gpt-5-mini Standard Model Output ===\n",
      "\n",
      "We have three people (Alice, Bob, Carol), three pets (cat, dog, fish) and three colors (red, blue, green). Use the clues one by one.\n",
      "\n",
      "Clues:\n",
      "1. Alice â‰  cat.\n",
      "2. Dog â†’ blue (the person with the dog likes blue).\n",
      "3. Carol = green.\n",
      "4. Bob â‰  fish.\n",
      "\n",
      "From (3) Carol likes green. From (2) the dog-person likes blue, so the dog-person cannot be Carol (green â‰  blue). Thus the dog is either Alice or Bob.\n",
      "\n",
      "Do a short case analysis:\n",
      "\n",
      "Case 1 â€” Dog = Bob:\n",
      "- Then Bob's color = blue (by 2).\n",
      "- Bob â‰  fish (clue 4), so Bob as dog is fine.\n",
      "- Remaining pets for Alice and Carol are cat and fish. Alice â‰  cat (clue 1), so Alice = fish and Carol = cat.\n",
      "- Colors: Bob = blue, Carol = green (given), so Alice must be red.\n",
      "Solution: Alice â€” fish, red; Bob â€” dog, blue; Carol â€” cat, green.\n",
      "\n",
      "Case 2 â€” Dog = Alice:\n",
      "- Then Alice's color = blue (by 2).\n",
      "- Alice â‰  cat (clue 1) â€” having the dog is consistent.\n",
      "- Bob â‰  fish (clue 4), so Bob cannot be fish; with Alice = dog, the remaining pets are cat and fish, so Bob must be cat and Carol must be fish.\n",
      "- Colors: Alice = blue, Carol = green (given), so Bob must be red.\n",
      "Solution: Alice â€” dog, blue; Bob â€” cat, red; Carol â€” fish, green.\n",
      "\n",
      "Both cases satisfy all clues. Therefore the puzzle does not have a unique solution; the two valid solutions are:\n",
      "\n",
      "- Solution A: Alice = fish/red, Bob = dog/blue, Carol = cat/green.\n",
      "- Solution B: Alice = dog/blue, Bob = cat/red, Carol = fish/green.\n",
      "\n",
      "=== Observations ===\n",
      "- Did both models get the correct answer?\n",
      "Yes, both models got the rigth answer.\n",
      "\n",
      "- Does o3-mini show step-by-step reasoning?\n",
      "Yes, it does.\n",
      "\n",
      "- How does gpt-5-mini's output differ in clarity or detail?\n",
      "reasoning is not as clear as o3-mini's.\n"
     ]
    }
   ],
   "source": [
    "# Task 7b: Compare Reasoning vs Non-Reasoning (5 pts)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Send the same logic_puzzle to gpt-5-mini\n",
    "# 2. Compare the response to o3-mini's response\n",
    "\n",
    "response_standard = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": logic_puzzle}]\n",
    ")\n",
    "\n",
    "# Print and compare:\n",
    "# - Did both models get the correct answer?\n",
    "# - How did their reasoning processes differ?\n",
    "# - Which response was more helpful/clear?\n",
    "print(\"\\n=== gpt-5-mini Standard Model Output ===\\n\")\n",
    "print(response_standard.choices[0].message.content)\n",
    "\n",
    "print(\"\\n=== Observations ===\")\n",
    "print(\"- Did both models get the correct answer?\")\n",
    "print(\"Yes, both models got the rigth answer.\")\n",
    "print()\n",
    "print(\"- Does o3-mini show step-by-step reasoning?\")\n",
    "print(\"Yes, it does.\")\n",
    "print()\n",
    "print(\"- How does gpt-5-mini's output differ in clarity or detail?\")\n",
    "print(\"reasoning is not as clear as o3-mini's.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 8: Generate Movie Poster (20 points)\n",
    "\n",
    "Now for the fun part - generating movie posters using AI!\n",
    "\n",
    "### 8a. Design a prompt generator (5 pts)\n",
    "\n",
    "Write a function that takes a `Movie` object and creates a detailed image generation prompt.\n",
    "\n",
    "**Your prompt should incorporate:**\n",
    "- The movie's visual style\n",
    "- The mood/tone\n",
    "- Key visual elements that represent the genre\n",
    "- Professional movie poster composition\n",
    "\n",
    "**Tip:** Aim for 50-100 words. Be specific about colors, composition, and style!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8a: Design your prompt generator (5 pts)\n",
    "\n",
    "def generate_poster_prompt(movie: Movie) -> str:\n",
    "    \"\"\"\n",
    "    Create a detailed image generation prompt from movie data.\n",
    "    \n",
    "    Returns a detailed prompt string (aim for 50-100 words)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - design your prompt template\n",
    "    # Consider: How can you use the movie's mood, visual_style, and genre\n",
    "    # to create an evocative image prompt?\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your prompt generator\n",
    "chosen_movie = movies[0]  # or pick your favorite from the list!\n",
    "prompt = generate_poster_prompt(chosen_movie)\n",
    "\n",
    "print(f\"Prompt for '{chosen_movie.title}':\")\n",
    "print()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Generate the actual image (10 pts)\n",
    "\n",
    "Use Google's Gemini to generate the movie poster.\n",
    "\n",
    "**Hints:**\n",
    "- Use `genai.Client()` to create a client\n",
    "- Use `client.models.generate_content()` with `model=\"gemini-2.5-flash-image\"`\n",
    "- The response will have an image in `response.candidates[0].content.parts`\n",
    "- Save the image to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8b: Generate the movie poster (10 pts)\n",
    "\n",
    "# Create Google client\n",
    "google_client = genai.Client()\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Generate the image using gemini-2.5-flash-image\n",
    "# 2. Extract the image from the response\n",
    "# 3. Save it to temp/poster_{movie_title}.png\n",
    "#    (Create the temp directory if it doesn't exist)\n",
    "\n",
    "# Make sure to create temp directory\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "\n",
    "# Generate and save your poster here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Display the image (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8c: Display the saved image (5 pts)\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# YOUR CODE HERE - display the poster you saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 9: Submit via Pull Request (15 points)\n",
    "\n",
    "Now let's practice a real-world development workflow! Instead of pushing directly to `main`, you'll create a **branch**, open a **Pull Request (PR)**, and **merge** it.\n",
    "\n",
    "This is how professional developers submit code for review. Your TA will check your merged PR to verify your submission.\n",
    "\n",
    "### 9a. Create a new branch (5 pts)\n",
    "\n",
    "Run this command in your terminal to create and switch to a new branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9a: Create a new branch (5 pts)\n",
    "# Run this in your terminal (not in this notebook!)\n",
    "\n",
    "!git checkout -b homework-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Commit your work (5 pts)\n",
    "\n",
    "Stage all your changes and create a commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9b: Commit your work (5 pts)\n",
    "\n",
    "!git add .\n",
    "!git commit -m \"Complete homework 2: Movie Poster Generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9c: Push your branch (5 pts)\n",
    "\n",
    "!git push -u origin homework-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9d. Create and Merge the Pull Request\n",
    "\n",
    "Now go to your repository on GitHub (https://github.com/YOUR-USERNAME/ai-engineering-fordham):\n",
    "\n",
    "1. You should see a banner saying **\"homework-2 had recent pushes\"** - click **\"Compare & pull request\"**\n",
    "2. Give your PR a title: `\"Homework 2: Movie Poster Generator\"`\n",
    "3. Click **\"Create pull request\"**\n",
    "4. Review your changes in the PR\n",
    "5. Click **\"Merge pull request\"** then **\"Confirm merge\"**\n",
    "\n",
    "**Your PR should now show as \"Merged\"** - this is what the TA will check!\n",
    "\n",
    "Run the cell below to verify your branch was merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your PR was merged (run after merging on GitHub)\n",
    "!git checkout main\n",
    "!git pull\n",
    "!git log --oneline -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BONUS: Full Pipeline (10 bonus points)\n",
    "\n",
    "Put everything together! Create a complete pipeline that takes a movie description and returns both the structured data AND a generated poster.\n",
    "\n",
    "**Challenge:** Write your own original movie description and generate a poster for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Create a complete pipeline (10 bonus pts)\n",
    "\n",
    "async def movie_to_poster(description: str) -> tuple[Movie, str]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: description -> structured data -> poster\n",
    "    \n",
    "    Args:\n",
    "        description: A text description of a movie\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (Movie object, path to saved poster image)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with YOUR OWN original movie idea!\n",
    "\n",
    "my_movie_description = \"\"\"\n",
    "YOUR ORIGINAL MOVIE IDEA HERE - BE CREATIVE!\n",
    "Describe the plot, characters, setting, visual style, and mood.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to run:\n",
    "# movie, poster_path = await movie_to_poster(my_movie_description)\n",
    "# print(f\"Generated poster for: {movie.title}\")\n",
    "# print(movie.model_dump_json(indent=2))\n",
    "# display(Image(poster_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Your `Movie` schema includes all required fields with proper validation\n",
    "- [ ] `extract_movie()` returns a valid `Movie` object\n",
    "- [ ] Async processing works and shows timing\n",
    "- [ ] Temperature comparison shows deterministic vs random outputs\n",
    "- [ ] Logprobs visualization works and displays token probabilities\n",
    "- [ ] Reasoning model comparison shows differences between o3-mini and gpt-5-mini\n",
    "- [ ] You generated and displayed at least one movie poster\n",
    "- [ ] Created branch `homework-2` and pushed to GitHub\n",
    "- [ ] Opened a Pull Request from `homework-2` to `main`\n",
    "- [ ] **Merged the PR** (it should show as \"Merged\" on GitHub)\n",
    "- [ ] Submitted notebook on Blackboard\n",
    "\n",
    "**Submission:**\n",
    "1. Complete all tasks in this notebook\n",
    "2. Create a PR and **merge it** on GitHub\n",
    "3. Submit your notebook (`.ipynb` file) on **Blackboard**\n",
    "\n",
    "**The TA will verify your submission by checking the merged PR on your GitHub repo.**\n",
    "\n",
    "---\n",
    "\n",
    "**Great work!** You've built a complete AI-powered application, explored LLM parameters and reasoning, and learned a professional Git workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
